{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# File QA RAG Chatbot App with ChatGPT, LangChain and Streamlit\n",
        "\n",
        "Here we will implement an advanced RAG System with ChatGPT, LangChain and Streamlit to build a File QA UI-based chatbot with the following features:\n",
        "\n",
        "PDF Document Upload and Indexing\n",
        "\n",
        "RAG System for query analysis and response\n",
        "\n",
        "Result streaming capabilities (Real-time output)\n",
        "\n",
        "Show document sources of the answer from RAG system"
      ],
      "metadata": {
        "id": "whf48tnnYQZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "KN3qX5BFYS6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwStu0hwVVvz"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.12\n",
        "!pip install langchain-openai==0.0.8\n",
        "!pip install langchain-community==0.0.29\n",
        "!pip install streamlit==1.32.2\n",
        "!pip install PyMuPDF==1.24.0\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install pyngrok==7.1.5\n",
        "\n",
        "!pip install PyPDF2 spacy pandas matplotlib wordcloud\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load OpenAI API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "2_2iLRwbYShw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "A8NX5Lxhu24w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Environment Variable"
      ],
      "metadata": {
        "id": "VIARBf60YbpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "CybYp_RcvHMw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write App Code Header"
      ],
      "metadata": {
        "id": "EGVqiywiYd70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4VoNtQtQjJNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter  # Changed to CharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from operator import itemgetter\n",
        "from os.path import basename\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"Dream Journal QA Chatbot\", page_icon=\"ðŸŒ™\")\n",
        "st.title(\"Dream Journal QA Chatbot ðŸŒ™\")\n",
        "st.sidebar.header(\"Document Upload and Dream Analysis\")\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")  # Stores uploaded documents for 1h in cache\n",
        "def configure_retriever(uploaded_files):\n",
        "    # Read documents\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploaded_files:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyMuPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Split into document chunks using a custom separator\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"/end\",  # Split specifically by /end marker\n",
        "        chunk_overlap=200,\n",
        "        keep_separator=True,  # Keep the separator in the chunks\n",
        "        chunk_size=2000      # Chunk size limit, but /end is primary separator\n",
        "    )\n",
        "    doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create document embeddings and store in a Vector DB\n",
        "    embeddings_model = OpenAIEmbeddings()\n",
        "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
        "\n",
        "    # Define retriever object\n",
        "    retriever = vectordb.as_retriever()\n",
        "    return retriever, doc_chunks  # Return both retriever and doc_chunks\n",
        "\n",
        "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "# Create UI element to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    label=\"Upload Dream Journal PDF files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "if not uploaded_files:\n",
        "    st.info(\"Please upload your Dream Journal PDF documents to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "# Create retriever object based on uploaded PDFs\n",
        "retriever, doc_chunks = configure_retriever(uploaded_files)  # Get both retriever and doc_chunks\n",
        "\n",
        "# Load a connection to ChatGPT LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1, streaming=True)\n",
        "\n",
        "# --- Question Type Classifier Chain ---\n",
        "question_type_prompt_template = \"\"\"\n",
        "Determine the type of question being asked. Choose from the following categories: count_question, full_dream_question, analysis_question, or qa_question.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Respond with 'count_question' if the question asks for a count of dreams related to a specific topic (e.g., \"how many dreams about cats?\").\n",
        "Respond with 'full_dream_question' if the question asks for the complete text of a dream (e.g., \"show me the full dream\", \"what is the dream about?\").\n",
        "Respond with 'analysis_question' if the question is a general request to analyze ALL dreams in the journal and provide an overall analysis or visualization, like \"analyze my dreams\", \"dream analysis\", \"plot dream words\", \"show me a word frequency plot of my dreams\". This is for a holistic analysis of the entire dream journal.\n",
        "Respond with 'qa_question' if the question is a general question asking for specific information or details ABOUT the content of the dreams, requiring a detailed answer from the dream journal (e.g., \"what happens in dreams about cats?\", \"tell me about dreams involving dogs\"). This is for question-answering about dream content.\n",
        "\n",
        "Just answer 'count_question', 'full_dream_question', 'analysis_question', or 'qa_question'.\n",
        "\"\"\"\n",
        "question_type_prompt = ChatPromptTemplate.from_template(question_type_prompt_template)\n",
        "question_type_chain = question_type_prompt | chatgpt\n",
        "\n",
        "# --- Topic Extraction Chain for Count Questions ---\n",
        "topic_extraction_prompt_template = \"\"\"\n",
        "User question: {question}\n",
        "\n",
        "Identify the topic that the user wants to count in their dream journal from the question above.\n",
        "Even if the question includes negation (like 'not about X'), identify 'X' as the topic.\n",
        "Just return the core topic, do not include any extra words or explanations, and do not include the negation words.\n",
        "If the question is too vague or it's not clear what to count, respond with 'unknown_topic'.\n",
        "\"\"\"\n",
        "topic_extraction_prompt = ChatPromptTemplate.from_template(topic_extraction_prompt_template)\n",
        "topic_extraction_chain = topic_extraction_prompt | chatgpt\n",
        "\n",
        "# --- QA RAG Chain ---\n",
        "qa_template = \"\"\"\n",
        "Use only the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know,\n",
        "don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | qa_prompt\n",
        "    | chatgpt\n",
        ")\n",
        "\n",
        "# --- Helper Function: Generate Topic Variations ---\n",
        "def get_topic_variations(topic, nlp_model):\n",
        "    \"\"\"\n",
        "    Using spaCy lemmatization, generate a set of topic variations\n",
        "    including singular and plural forms.\n",
        "    \"\"\"\n",
        "    topic = topic.lower().strip()\n",
        "    doc = nlp_model(topic)\n",
        "    variations = set()\n",
        "    for token in doc:\n",
        "        # Add the lemma (base form)\n",
        "        variations.add(token.lemma_)\n",
        "        # Add plural/singular variations:\n",
        "        if token.lemma_.endswith(\"s\"):\n",
        "            variations.add(token.lemma_[:-1])\n",
        "        else:\n",
        "            variations.add(token.lemma_ + \"s\")\n",
        "    # Always include the original topic as provided\n",
        "    variations.add(topic)\n",
        "    return list(variations)\n",
        "\n",
        "# --- Updated Counting Dreams by Topic Function ---\n",
        "def count_dreams_by_topic(document_chunks, topic, negate=False):\n",
        "    \"\"\"\n",
        "    Analyze each dream chunk to see if it (or its page) is about a dream that\n",
        "    includes any of the variations of the topic. For non-negated queries the prompt\n",
        "    instructs the LLM to determine if the dream is about any of these topics,\n",
        "    and for negated queries it instructs the LLM to determine if the dream is NOT about them.\n",
        "    The function ensures that the same page is not counted twice.\n",
        "    \"\"\"\n",
        "    dream_count = 0\n",
        "    source_documents_count = []  # To store source document info for count questions\n",
        "    counted_pages = set()        # To avoid counting the same page twice\n",
        "\n",
        "    # Generate topic variations using spaCy\n",
        "    topic_variations = get_topic_variations(topic, nlp_model)\n",
        "    variations_text = \", \".join(topic_variations)\n",
        "\n",
        "    with st.spinner(f\"Analyzing dreams for '{topic}'...\"):\n",
        "        for chunk in document_chunks:\n",
        "            source = chunk.metadata.get(\"source\", \"unknown\")\n",
        "            page = chunk.metadata.get(\"page\", \"unknown\")\n",
        "            page_id = (basename(source), page)\n",
        "            if page_id in counted_pages:\n",
        "                continue  # Skip if this page has already been counted\n",
        "\n",
        "            if negate:\n",
        "                # For negated queries, use simpler wording\n",
        "                prompt = f\"\"\"Analyze the following dream journal entry and determine if it is NOT about a dream that includes any of the following topics: {variations_text}.\n",
        "Respond with 'yes' if the dream is NOT about any of these topics, and 'no' if it is about one or more of them. Just answer 'yes' or 'no'.\n",
        "\n",
        "Dream Entry:\n",
        "{chunk.page_content}\n",
        "\"\"\"\n",
        "            else:\n",
        "                prompt = f\"\"\"Analyze the following dream journal entry and determine if it is about a dream that includes any of the following topics: {variations_text}.\n",
        "Respond with 'yes' if the dream is about one or more of these topics, and 'no' if it is not. Just answer 'yes' or 'no'.\n",
        "\n",
        "Dream Entry:\n",
        "{chunk.page_content}\n",
        "\"\"\"\n",
        "            response = chatgpt.invoke(prompt)\n",
        "            llm_response_content = response.content.strip().lower()\n",
        "            if llm_response_content == \"yes\":\n",
        "                dream_count += 1\n",
        "                counted_pages.add(page_id)\n",
        "                source_documents_count.append({\n",
        "                    \"source\": basename(source),\n",
        "                    \"page\": page,\n",
        "                    \"content\": chunk.page_content[:200] + \"...\"\n",
        "                })\n",
        "    return dream_count, source_documents_count\n",
        "\n",
        "# --- Function to display source documents table ---\n",
        "def display_source_documents_table(source_documents):\n",
        "    if source_documents:\n",
        "        st.markdown(\"__Dreams related to the topic (Sources):__\" + \"\\n\")\n",
        "        st.dataframe(data=pd.DataFrame(source_documents), width=1000)\n",
        "\n",
        "# --- Function to retrieve and display full dream text ---\n",
        "def retrieve_full_dream_text(user_question, document_chunks, retriever):\n",
        "    relevant_docs = retriever.get_relevant_documents(user_question)\n",
        "    if not relevant_docs:\n",
        "        return \"No dream found related to your question.\"\n",
        "\n",
        "    # Group chunks by source and page to reconstruct the full dream\n",
        "    dream_pages = {}\n",
        "    for doc in relevant_docs:\n",
        "        source = basename(doc.metadata['source'])\n",
        "        page = doc.metadata['page']\n",
        "        key = (source, page)\n",
        "        if key not in dream_pages:\n",
        "            dream_pages[key] = []\n",
        "        dream_pages[key].append(doc)\n",
        "\n",
        "    full_dreams_text = \"\"\n",
        "    for key in dream_pages:\n",
        "        page_chunks = sorted(dream_pages[key], key=lambda doc: doc.metadata.get('page', 0))\n",
        "        dream_text_parts = [chunk.page_content for chunk in page_chunks]\n",
        "        full_dreams_text += \"\\n\\n\".join(dream_text_parts)\n",
        "    return full_dreams_text.strip()\n",
        "\n",
        "# --- NLP Analysis and Plotting Functions ---\n",
        "@st.cache_resource\n",
        "def load_spacy_model():\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"Downloading en_core_web_sm model...\")\n",
        "        spacy.cli.download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "    return nlp\n",
        "\n",
        "nlp_model = load_spacy_model()\n",
        "\n",
        "def analyze_dreams_spacy(dream_texts, nlp):\n",
        "    all_dream_text = \" \".join(dream_texts)\n",
        "    doc = nlp(all_dream_text)\n",
        "    tokens = [\n",
        "        token.text.lower() for token in doc\n",
        "        if not token.is_stop and not token.is_punct and len(token.text) > 2\n",
        "           and token.text.lower() not in [\"/end\", \"dream\", \"enddream\"]\n",
        "    ]\n",
        "    word_freq = Counter(tokens)\n",
        "    return word_freq\n",
        "\n",
        "def plot_horizontal_word_frequencies(sorted_word_freq, top_n=50):\n",
        "    words, frequencies = zip(*sorted_word_freq[:top_n])\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    plt.barh(words, frequencies, color='skyblue')\n",
        "    plt.xlabel('Frequency', fontsize=12)\n",
        "    plt.ylabel('Words', fontsize=12)\n",
        "    plt.title(f'Top {top_n} Word Frequencies in Dreams', fontsize=14)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    buffer = BytesIO()\n",
        "    plt.savefig(buffer, format='png')\n",
        "    plt.close()\n",
        "    buffer.seek(0)\n",
        "    return buffer\n",
        "\n",
        "def analyze_and_plot_dreams(document_chunks, top_n):\n",
        "    dream_texts = [chunk.page_content for chunk in document_chunks]\n",
        "    if not dream_texts:\n",
        "        return None, \"No dream texts found to analyze.\"\n",
        "    dream_word_freq = analyze_dreams_spacy(dream_texts, nlp_model)\n",
        "    sorted_dream_word_freq = sorted(dream_word_freq.items(), key=lambda item: item[1], reverse=True)\n",
        "    if not sorted_dream_word_freq:\n",
        "        return None, \"No significant words found for plotting.\"\n",
        "    plot_buffer = plot_horizontal_word_frequencies(sorted_dream_word_freq, top_n=top_n)\n",
        "    return plot_buffer, None\n",
        "\n",
        "# --- PostMessageHandler: Ensure Unique Source Documents ---\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "    def __init__(self, msg: st.write):\n",
        "        super().__init__()\n",
        "        self.msg = msg\n",
        "        self.sources = []  # This will store unique source metadata\n",
        "\n",
        "    def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "        for d in documents:\n",
        "            # Use basename to remove temporary directory differences\n",
        "            src = basename(d.metadata[\"source\"])\n",
        "            metadata = {\n",
        "                \"source\": src,\n",
        "                \"page\": d.metadata[\"page\"],\n",
        "                \"content\": d.page_content[:200]\n",
        "            }\n",
        "            idx = (src, d.metadata[\"page\"])\n",
        "            # Check if this (source, page) tuple is already present\n",
        "            if idx not in {(s[\"source\"], s[\"page\"]) for s in self.sources}:\n",
        "                self.sources.append(metadata)\n",
        "\n",
        "    def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "        if self.sources:\n",
        "            st.markdown(\"__Sources:__\" + \"\\n\")\n",
        "            st.dataframe(data=pd.DataFrame(self.sources[:3]), width=1000)\n",
        "\n",
        "# --- Conversation Handling ---\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "if \"awaiting_top_n\" not in st.session_state:\n",
        "    st.session_state.awaiting_top_n = False\n",
        "if \"analysis_prompt\" not in st.session_state:\n",
        "    st.session_state.analysis_prompt = None\n",
        "\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "    streamlit_msg_history.add_ai_message(\"Please ask questions about your dream journal, including count-related questions, general questions, requests to retrieve the full text of a dream or ask for analysis and plots.\")\n",
        "\n",
        "for msg in streamlit_msg_history.messages:\n",
        "    st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "if user_prompt := st.chat_input():\n",
        "    st.chat_message(\"human\").write(user_prompt)\n",
        "\n",
        "    if st.session_state.awaiting_top_n:\n",
        "        try:\n",
        "            top_n_words = int(user_prompt)\n",
        "            if top_n_words <= 0:\n",
        "                st.error(\"Please enter a positive number for the number of top words.\")\n",
        "                streamlit_msg_history.add_user_message(user_prompt)\n",
        "                streamlit_msg_history.add_ai_message(\"Please enter a positive number for the number of top words.\")\n",
        "            else:\n",
        "                st.session_state.awaiting_top_n = False\n",
        "                plot_buffer, error_message = analyze_and_plot_dreams(doc_chunks, top_n_words)\n",
        "                with st.chat_message(\"ai\"):\n",
        "                    if plot_buffer:\n",
        "                        st.image(plot_buffer, caption=f'Top {top_n_words} Word Frequency Plot of Dreams')\n",
        "                        st.markdown(f\"Below is a word frequency plot of the top {top_n_words} most common words in your dream journal. This visualization helps to identify recurring themes and patterns in your dreams.\")\n",
        "                        streamlit_msg_history.add_ai_message(f\"Here is a word frequency plot of your dreams showing top {top_n_words} words:\")\n",
        "                    else:\n",
        "                        error_text = error_message or \"Failed to generate dream analysis plot.\"\n",
        "                        st.error(error_text)\n",
        "                        streamlit_msg_history.add_ai_message(error_text)\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "        except ValueError:\n",
        "            st.error(\"Invalid input. Please enter a number.\")\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(\"Invalid input. Please enter a number.\")\n",
        "            st.session_state.awaiting_top_n = True\n",
        "\n",
        "    else:\n",
        "        # --- Classify Question Type ---\n",
        "        question_type_response = question_type_chain.invoke({\"question\": user_prompt})\n",
        "        question_type = question_type_response.content.strip()\n",
        "\n",
        "        if question_type == \"count_question\":\n",
        "            # --- Handle Count Question ---\n",
        "            topic_extraction_response = topic_extraction_chain.invoke({\"question\": user_prompt})\n",
        "            topic_to_count = topic_extraction_response.content.strip()\n",
        "\n",
        "            if topic_to_count == \"unknown_topic\":\n",
        "                st.warning(\"Sorry, I couldn't understand what topic you want to count. Please be more specific.\")\n",
        "                streamlit_msg_history.add_user_message(user_prompt)\n",
        "                streamlit_msg_history.add_ai_message(\"Sorry, I couldn't understand what topic you want to count. Please be more specific.\")\n",
        "            else:\n",
        "                negated_question = False\n",
        "                negation_words = [\"not\", \"no\", \"without\", \"excluding\"]\n",
        "                if any(neg_word in user_prompt.lower() for neg_word in negation_words):\n",
        "                    negated_question = True\n",
        "\n",
        "                dream_count, source_documents_count = count_dreams_by_topic(doc_chunks, topic_to_count, negate=negated_question)\n",
        "                if negated_question:\n",
        "                    answer_text = f\"I have analyzed your dream journal and found **{dream_count} dreams that are NOT about '{topic_to_count}'**.\"\n",
        "                else:\n",
        "                    answer_text = f\"I have analyzed your dream journal and found **{dream_count} dreams about '{topic_to_count}'**.\"\n",
        "                st.chat_message(\"ai\").write(answer_text)\n",
        "                streamlit_msg_history.add_ai_message(answer_text)\n",
        "                display_source_documents_table(source_documents_count)\n",
        "                streamlit_msg_history.add_user_message(user_prompt)\n",
        "\n",
        "        elif question_type == \"qa_question\":\n",
        "            # --- Handle QA Question (Normal RAG flow) ---\n",
        "            with st.chat_message(\"ai\"):\n",
        "                stream_handler = StreamHandler(st.empty())\n",
        "                sources_container = st.write(\"\")\n",
        "                pm_handler = PostMessageHandler(sources_container)\n",
        "                config = {\"callbacks\": [stream_handler, pm_handler]}\n",
        "                response = qa_rag_chain.invoke({\"question\": user_prompt}, config)\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(response.content)\n",
        "\n",
        "        elif question_type == \"full_dream_question\":\n",
        "            # --- Handle Full Dream Retrieval ---\n",
        "            with st.chat_message(\"ai\"):\n",
        "                full_dream_text = retrieve_full_dream_text(user_prompt, doc_chunks, retriever)\n",
        "                cleaned_dream_text = full_dream_text.split('/end')[0].strip()\n",
        "                st.markdown(cleaned_dream_text)\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(cleaned_dream_text)\n",
        "\n",
        "        elif question_type == \"analysis_question\":\n",
        "            # --- Handle Dream Analysis and Plot ---\n",
        "            st.session_state.awaiting_top_n = True\n",
        "            st.session_state.analysis_prompt = user_prompt\n",
        "            with st.chat_message(\"ai\"):\n",
        "                st.write(\"How many top words would you like to see in the plot?\")\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(\"How many top words would you like to see in the plot?\")\n",
        "\n",
        "        else:\n",
        "            st.chat_message(\"ai\").write(\"Sorry, I could not understand the type of question. Please rephrase.\")\n",
        "            streamlit_msg_history.add_user_message(user_prompt)\n",
        "            streamlit_msg_history.add_ai_message(\"Sorry, I could not understand the type of question. Please rephrase.\")\n"
      ],
      "metadata": {
        "id": "vbh76Dxzp9HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the Streamlit App"
      ],
      "metadata": {
        "id": "NQTrcbMsv2Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>/./logs.txt &"
      ],
      "metadata": {
        "id": "zBVCZkF7v3R1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up ngrok Tunnel"
      ],
      "metadata": {
        "id": "-AfJYjmXv4-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "ngrok_auth_token = getpass('Enter ngrok API Key: ')"
      ],
      "metadata": {
        "id": "AkoLJ7BZOWaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Authenticate ngrok with the token read from the file\n",
        "!ngrok config add-authtoken {ngrok_auth_token}\n",
        "\n",
        "# Open an HTTPS tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "blbhpK7uwSf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}